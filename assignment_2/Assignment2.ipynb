{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvigâ€™s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nordwig baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from https://norvig.com/spell-correct.html\n",
    "\n",
    "\n",
    "def candidates(word, vocab):\n",
    "    \"\"\"\n",
    "    :param word: token to be corrected\n",
    "    :param vocab: set of known words\n",
    "    :return: possible spelling corrections for word \n",
    "    \"\"\"\n",
    "    return known([word], vocab) or known(edits1(word), vocab) or known(edits2(word), vocab) or [word]\n",
    "\n",
    "\n",
    "def known(words, vocab):\n",
    "    \"\"\"\n",
    "    :param words: list of tokens\n",
    "    :param vocab: set of known tokens\n",
    "    :return: subset of words which are in vocab\n",
    "    \"\"\"\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in vocab)\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    :param word: token to be altered\n",
    "    :return: set of all words within Damerau-Levenstein distance radius 1\n",
    "    \"\"\"\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"\"\"\n",
    "    :param word: token to be altered\n",
    "    :return: set of all words within Damerau-Levenstein distance radius 2\n",
    "    \"\"\"\n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "class NordwigCorrector:\n",
    "    def __init__(self, lines):\n",
    "        \"\"\"\n",
    "        :param lines: list of space-separated tokens\n",
    "        \"\"\"\n",
    "        words = ' '.join(lines).split()\n",
    "        self.words = Counter(words)\n",
    "        self.N = sum(self.words.values())\n",
    "\n",
    "    def P(self, word):\n",
    "        \"\"\"\n",
    "        Estimate the marginal token probability in the known corpora\n",
    "        :param word: str, a single token \n",
    "        :return: P(word)\n",
    "        \"\"\"\n",
    "        return self.words[word] / self.N\n",
    "\n",
    "    def word_correction(self, word):\n",
    "        \"\"\"\n",
    "        :param word: token with a spelling error\n",
    "        :return: The correction (within distance of at most 2 from word) with the highest marginal probability\n",
    "        \"\"\"\n",
    "        return max(candidates(word, self.words), key=self.P)\n",
    "\n",
    "    def correction(self, sentence):\n",
    "        \"\"\"\n",
    "        :param sentence: str, space-separated tokens\n",
    "        :return: sentence with corrected spelling errors\n",
    "        \"\"\"\n",
    "        words = sentence.lower().split()\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in self.words:\n",
    "                words[i] = self.word_correction(words[i])\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code has been adapted from https://github.com/yandexdataschool/nlp_course/tree/2023/week03_lm\n",
    "\n",
    "UNK = \"_UNK_\"\n",
    "\n",
    "\n",
    "class KneserNeyLanguageModel:\n",
    "    def __init__(self, data_or_path, n=3, kn_delta: float = 1.0):\n",
    "        \"\"\"\n",
    "        :param data_or_path: if str given, should be a path to a file with lines having the following structure:\n",
    "            \"occurrence_number word1 word2 ... \"\n",
    "            Otherwise, should be a tuple of two lists: lines of space-separated tokens and occurrence counts for them\n",
    "        :param n: n for n-gram modeling\n",
    "        :param kn_delta: delta used in Kneser-Ney smoothing\n",
    "        \"\"\"\n",
    "        assert type(n) is int and 2 <= n <= 5, 'N for N-Grams should be an integer from [2; 5]'\n",
    "        self.n = n - 1\n",
    "        self.counts = {}\n",
    "        if type(data_or_path) is str:\n",
    "            lines = []\n",
    "            line_counts = []\n",
    "            with open(data_or_path, 'r') as dataset:\n",
    "                for line in dataset.readlines():\n",
    "                    count, *words = line.strip().split()\n",
    "                    line_counts.append(int(count))\n",
    "                    lines.append(' '.join(words))\n",
    "        else:\n",
    "            lines, line_counts = data_or_path\n",
    "        for prefix_len in tqdm(range(self.n + 1), desc='Training LM', leave=False):\n",
    "            bare_counts = self.count_ngrams(lines, line_counts, prefix_len + 1)\n",
    "            self.counts[prefix_len] = bare_counts\n",
    "        self.kn_delta = kn_delta\n",
    "        self.vocab = set(token for token_counts in self.counts[0].values() for token in token_counts)\n",
    "\n",
    "    def get_possible_next_tokens(self, prefix, candidates):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        :param prefix: str, space-separated tokens\n",
    "        :param candidates: list of strings - single tokens\n",
    "        :return: dict { candidate: P(candidate | prefix) }\n",
    "        \"\"\"\n",
    "        return {token: self.get_next_token_prob(prefix, token) for token in candidates}\n",
    "\n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        \"\"\"\n",
    "        :param prefix: str, space-separated tokens (can be empty)\n",
    "        :param next_token: str, single token\n",
    "        :return: P(next_token | prefix)\n",
    "        \"\"\"\n",
    "        prefix = prefix.split()\n",
    "        p = self.kn_delta / len(self.vocab)\n",
    "        for prefix_len in range(min(self.n, len(prefix)) + 1):\n",
    "            key_prefix = tuple(prefix[-prefix_len:]) if prefix_len > 0 else ()\n",
    "            new = self.counts[prefix_len][key_prefix]\n",
    "            norm = sum(new.values())\n",
    "            if norm == 0:\n",
    "                break\n",
    "            n_different = len(set([token for token in new.keys() if new[token] > 0]))\n",
    "            p = max(0, new[next_token] - self.kn_delta) / norm + (p * self.kn_delta * n_different) / norm\n",
    "        return p\n",
    "\n",
    "    def correction(self, sentence):\n",
    "        \"\"\"\n",
    "        Iterate over tokens in the sentence, if a word is not in the LM vocab, then iterate over words within \n",
    "        Damerau-Levenstein distance of at most 2, rank these words by probability of occurring in the left context\n",
    "        Choose greedily the word with the largest probability\n",
    "        :param sentence: str, space-separated tokens\n",
    "        :return: sentence with corrected spelling errors\n",
    "        \"\"\"\n",
    "        prefix = ''\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word not in self.vocab:\n",
    "                dl_closest_words = self.get_possible_next_tokens(prefix, candidates(word, self.vocab)).items()\n",
    "                largest_likelihood_word = sorted(list(dl_closest_words), key=lambda x: x[1])[-1][0]\n",
    "                correct_word = largest_likelihood_word\n",
    "            else:\n",
    "                correct_word = word\n",
    "            prefix += correct_word + ' '\n",
    "        return prefix[:-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def count_ngrams(lines, line_counts, n):\n",
    "        \"\"\"\n",
    "        :param lines: an iterable of strings with space-separated tokens\n",
    "        :param line_counts: list of integers corresponding to line counts\n",
    "        :param n: n in n-gram to be used when counting\n",
    "        :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
    "        \"\"\"\n",
    "        counts = defaultdict(Counter)\n",
    "        # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
    "        for t in range(len(lines)):\n",
    "            line = lines[t].split()\n",
    "            for i in range(len(line)):\n",
    "                prefix = []\n",
    "                if i < n - 1:\n",
    "                    prefix = [UNK for _ in range(n - 1 - i)]\n",
    "                prefix.extend(line[max(0, i - n + 1): i])\n",
    "                counts[tuple(prefix)][line[i]] += line_counts[i]\n",
    "        return counts\n",
    "\n",
    "    def perplexity(self, lines, min_log_prob=np.log(10 ** -50.)):\n",
    "        \"\"\"\n",
    "        :param lines: a list of strings with space-separated tokens\n",
    "        :param min_log_prob: if log(P(w | ...)) is smaller than min_log_prob, set it equal to min_log_prob\n",
    "        :returns: corpora-level perplexity - a single scalar number\n",
    "        \"\"\"\n",
    "        log_PP = 0\n",
    "        N = 0\n",
    "        for line in tqdm(lines, desc='Evaluating perplexity', leave=False):\n",
    "            prefix = ''\n",
    "            line_split = line.split()\n",
    "            N += len(line_split)\n",
    "            for token in line_split:\n",
    "                prob = self.get_next_token_prob(prefix, token)\n",
    "                log_PP -= max(np.log(prob) if prob != 0 else -float('inf'), min_log_prob)\n",
    "                prefix = prefix + ' ' + token\n",
    "\n",
    "        return np.exp(log_PP / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kn_lm = KneserNeyLanguageModel('data/fivegrams.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 2.8659434390573218e-06,\n",
       " 'are': 0.9404379380602723,\n",
       " 'were': 0.008358404634502927,\n",
       " 'umbrella': 2.715145139936109e-10}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "kn_lm.get_possible_next_tokens('there they', ['is', 'are', 'were', 'umbrella'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this task is easy for me\n",
      "due to the lack of food\n",
      "kicked it with his foot\n"
     ]
    }
   ],
   "source": [
    "for sent in ['this tasg is esy fr me', 'due to the lack of foo', 'kicked it with his foo']:\n",
    "    print(kn_lm.correction(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holmes corpora preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmes_dataset = []\n",
    "\n",
    "with open('data/big.txt', 'r') as inp:\n",
    "    text = ''\n",
    "    for line in inp.readlines():\n",
    "        if line == '\\n':\n",
    "            if len(text):\n",
    "                holmes_dataset.append(text[:-1])\n",
    "            text = ''\n",
    "        else:\n",
    "            words = ' '.join(re.findall(r'\\w+', line.strip().lower()))\n",
    "            text += words + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21648"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holmes_dataset = [line for line in holmes_dataset if len(line) >= 5]\n",
    "len(holmes_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train-test split\n",
    "n_train_samples = int(len(holmes_dataset) * 0.9)\n",
    "train_dataset, test_dataset = holmes_dataset[:n_train_samples], holmes_dataset[n_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the train dataset to format acceptible by KneserNeyLanguageModel\n",
    "train_dataset_lines_counts = Counter()\n",
    "for line in train_dataset:\n",
    "    tokens = line.split()\n",
    "    for i in range(len(tokens) - 5 + 1):\n",
    "        train_dataset_lines_counts[' '.join(tokens[i:i+5])] += 1\n",
    "train_dataset = (list(train_dataset_lines_counts.keys()), list(train_dataset_lines_counts.values()),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of',\n",
       " 'project gutenberg ebook of the',\n",
       " 'gutenberg ebook of the adventures',\n",
       " 'ebook of the adventures of',\n",
       " 'of the adventures of sherlock',\n",
       " 'the adventures of sherlock holmes',\n",
       " 'adventures of sherlock holmes by',\n",
       " 'of sherlock holmes by sir',\n",
       " 'sherlock holmes by sir arthur',\n",
       " 'holmes by sir arthur conan']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c98101c7304382b00db0fcb94db278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "context iter:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta iter:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta iter:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating perplexity:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For N=2 Delta=0.1: Perplexity = 142.31279637520808\n",
      "For N=2 Delta=0.2: Perplexity = 143.21152642361074\n",
      "For N=2 Delta=0.3: Perplexity = 144.14151635607982\n",
      "For N=2 Delta=0.4: Perplexity = 145.10562358365482\n",
      "For N=2 Delta=0.5: Perplexity = 146.10768472599693\n",
      "For N=2 Delta=0.6: Perplexity = 147.15323909374723\n",
      "For N=2 Delta=0.7: Perplexity = 148.2512762906983\n",
      "For N=2 Delta=0.8: Perplexity = 149.41951560045982\n",
      "For N=2 Delta=0.9: Perplexity = 150.70846189948583\n",
      "For N=2 Delta=1.0: Perplexity = 153.3654424198026\n",
      "For N=3 Delta=0.1: Perplexity = 28.85392635792121\n",
      "For N=3 Delta=0.2: Perplexity = 29.28912042521709\n",
      "For N=3 Delta=0.3: Perplexity = 29.74875165498188\n",
      "For N=3 Delta=0.4: Perplexity = 30.235434063496594\n",
      "For N=3 Delta=0.5: Perplexity = 30.75246102013715\n",
      "For N=3 Delta=0.6: Perplexity = 31.30421318910851\n",
      "For N=3 Delta=0.7: Perplexity = 31.897096249170502\n",
      "For N=3 Delta=0.8: Perplexity = 32.54227303496812\n",
      "For N=3 Delta=0.9: Perplexity = 33.26777620325647\n",
      "For N=3 Delta=1.0: Perplexity = 34.751523315281716\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "validation_dataset = np.random.choice(train_dataset[0], 400, replace=False)\n",
    "for context_size in tqdm(range(2, 4), desc='context iter'):\n",
    "    for delta in tqdm(range(1, 10 + 1), desc='delta iter', leave=False):\n",
    "        _lm = KneserNeyLanguageModel(train_dataset, n=context_size, kn_delta = delta / 10)\n",
    "        pp = _lm.perplexity(validation_dataset)\n",
    "        history.append((context_size, delta / 10, pp))\n",
    "for context_size, delta, pp in history:\n",
    "    print(f'For N={context_size} Delta={delta}: Perplexity = {pp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "<!-- Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "- Inside my N-gram LM I used Kneser-Ney smoothing for higher robustness to rare n-grams and overall better generalization than that of no smoothing LM or Laplacian smoothing.\n",
    "- Optimal value for $\\delta$ and $N$ for the LM are found on train dataset by grid search: I have selected the parameters which gave the lowest perplexity on a subset of a training set\n",
    "- For N-gram dataset I have chosen the fivegrams dataset as one providing extensive statistics over long N-grams which are needed for backoff in Kneser-Ney approach, and for the benchmarking dataset - the Holmes corpora since it was compact and unlike the N-grams dataset a part of the Holmes corpora could be easily separated to be testing set.\n",
    "- My approach is an improvement of Nordwig's solution: instead of ranking corrections based only on $P(correction)$ derived from the frequency in the dataset, I evaluate $P(correction|context)$ based on the LM trained on the corpus. The statement of improvement is also supported by increased spelling correction top-1 accuracy (see below). The only flaw I have found thus far in my approach is slower inference and training time (almost 2 times slower when inferencing) if compared to Nordwig's solution.\n",
    "\n",
    "To sum up, Kneser-Ney outperforms the Nordwig's solution significantly in spelling errors correction, however falling short in inference speed. The benefit of LM-based solution is however the ability to make context-sensitive error correction, which can compensate for slower runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LM:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline = NordwigCorrector(train_dataset[0])\n",
    "best_kn_lm = KneserNeyLanguageModel(train_dataset, n=3, kn_delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(lines, vocab):\n",
    "    corrupted_lines = []\n",
    "    for line in tqdm(lines):\n",
    "        words = line.split()\n",
    "        n_typos = np.random.randint(low=1, high=3 + 1)\n",
    "        if n_typos > len(words):\n",
    "            n_typos = 0\n",
    "        typos_indices = np.random.choice(range(len(words)), size=n_typos, replace=False)\n",
    "        for typo_idx in typos_indices:\n",
    "            words[typo_idx] = random_corruption(words[typo_idx], vocab)\n",
    "        corrupted_lines.append(' '.join(words))\n",
    "    return corrupted_lines\n",
    "\n",
    "\n",
    "def random_corruption(word, vocab):\n",
    "    if np.random.uniform() < 0.2:\n",
    "        edit_function = edits2\n",
    "    else:\n",
    "        edit_function = edits1\n",
    "    return random.choice([c_word for c_word in edit_function(word) if c_word not in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db175473baa34451a889bb0edf881e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset_corrupt = prepare_dataset(test_dataset, best_kn_lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i am sure that rascal was lying said the count',\n",
       "  'i am sfre that rascal was lying said the count'),\n",
       " ('they can still be called back said one of his suite who like count orlov felt distrustful of the adventure when he looked at the enemy s camp',\n",
       "  'they can still be called back said one of his suite who like count orlov felt diutrustful of the adventure when he loored az the enemy s camp'),\n",
       " ('eh really what do you think should we let them go on or not',\n",
       "  'eh reallky what do you think should we lmt them go oin or not'),\n",
       " ('will you have them fetched back', 'willvm you have tqhem fetched back')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(test_dataset, test_dataset_corrupt))[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [],
   "source": [
    "def measure_quality(corrector, corrupt, gt, n_samples, results_prefix):\n",
    "    \"\"\"\n",
    "    Measure the ratio of corrected spelling errors to the total number of spelling errors\n",
    "    :param corrector: either Nordwig model, of KneserNey LM\n",
    "    :param corrupt: list of space-separated tokens, some of which contain typos\n",
    "    :param gt: list of correct space-separated tokens sentences\n",
    "    :param n_samples: Maximal number of lines to consider in evaluation\n",
    "    :param results_prefix: str, text to be printed before reporting the accuracy\n",
    "    :return: None, print the accuracy preceded by results_prefix\n",
    "    \"\"\"\n",
    "    corrected_mistakes = 0\n",
    "    total_mistakes = 0\n",
    "    for x, y in tqdm(zip(corrupt[:n_samples], gt[:n_samples]), total=n_samples):\n",
    "        y_hat = corrector.correction(x).split()\n",
    "        x, y = x.split(), y.split()\n",
    "        for i in range(len(x)):\n",
    "            corrected_mistakes += y[i] != x[i] and y[i] == y_hat[i]\n",
    "            total_mistakes += y[i] != x[i]\n",
    "        # corrected_mistakes += y == y_hat\n",
    "        # total_mistakes += 1\n",
    "    print(f'{results_prefix}: {corrected_mistakes / total_mistakes * 100:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96dcef7bea0a4bf094d88ff4af819d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nordwig solution accuracy: 80.884\n"
     ]
    }
   ],
   "source": [
    "measure_quality(baseline, test_dataset_corrupt, test_dataset, n_samples=len(test_dataset), results_prefix='Nordwig solution accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc2e11fc83642f08c3fa381e667e177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kneser-Ney LM accuracy: 85.066\n"
     ]
    }
   ],
   "source": [
    "measure_quality(best_kn_lm, test_dataset_corrupt, test_dataset, n_samples=len(test_dataset), results_prefix='Kneser-Ney LM accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
